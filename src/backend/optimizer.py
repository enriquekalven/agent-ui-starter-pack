import sys
import os
import time

def audit_agent(file_path):
    """
    Agent Optimizer CLI: Audits agent code and proposes cost/perf optimizations.
    Supports interactive [approve/reject] flow.
    """
    print("\n" + "="*50)
    print("ðŸ”  GCP AGENT OPS: OPTIMIZER AUDIT")
    print("="*50)
    print(f"Target: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ Error: File {file_path} not found.")
        return

    with open(file_path, 'r') as f:
        content = f.read()
    
    # Simulation logic for auditing
    token_count = len(content.split()) * 1.5 
    print(f"ðŸ“Š Token Metrics: ~{token_count:.0f} prompt tokens detected.")
    
    time.sleep(0.5)
    print("\nRunning heuristic analysis...")
    time.sleep(1)

    optimizations = [
        {
            "id": "context_caching",
            "title": "Enable Gemini Context Caching",
            "impact": "HIGH",
            "savings": "90% cost reduction on reuse",
            "description": "Your agent has a large static system instruction. Moving this to a context cache prevents redundant token processing.",
            "diff": "- # Standard model initialization\n+ # Using Context Caching (v2.0 Update)\n+ cache = vertexai.preview.CachingConfig(ttl=3600)\n+ model = GenerativeModel('gemini-1.5-pro', caching_config=cache)"
        },
        {
            "id": "prompt_compression",
            "title": "Semantic Prompt Compression",
            "impact": "MEDIUM",
            "savings": "15-20% token savings",
            "description": "Detected redundant boilerplate in A2UI generator. Can be compressed without loss of intent.",
            "diff": "- \"This UI was generated by a live Python backend agent using ADK patterns.\"\n+ \"Generated via Python backend using ADK.\""
        },
        {
            "id": "model_routing",
            "title": "Flash-First Model Routing",
            "impact": "CRITICAL",
            "savings": "10x lower latency & cost",
            "description": "Current routing logic maps simple queries to Pro. Swapping to Flash for these turns is recommended.",
            "diff": "@@ -41,1 +41,1 @@\n- model = 'gemini-1.5-pro'\n+ model = 'gemini-1.5-flash'"
        }
    ]

    applied = 0
    rejected = 0

    for opt in optimizations:
        print(f"\n--- [{opt['impact']} IMPACT] {opt['title']} ---")
        print(f"Benefit: {opt['savings']}")
        print(f"Reason: {opt['description']}")
        print(f"\nProposed Change:")
        print(f"\033[92m{opt['diff']}\033[0m")
        
        while True:
            choice = input(f"\nDo you want to apply this optimization? (approve/reject) [approve]: ").lower().strip()
            if choice in ["", "approve", "y", "yes"]:
                print(f"âœ… [APPROVED] queued for deployment.")
                applied += 1
                break
            elif choice in ["reject", "n", "no"]:
                print(f"âŒ [REJECTED] skipping optimization.")
                rejected += 1
                break
            else:
                print("Please enter 'approve' or 'reject'.")

    print("\n" + "="*50)
    print("ðŸŽ¯ AUDIT SUMMARY")
    print(f"Optimizations Applied:  {applied}")
    print(f"Optimizations Rejected: {rejected}")
    
    if applied > 0:
        print("\nðŸš€ Ready for production. Run 'make deploy-prod' to push changes.")
    else:
        print("\nâš ï¸  No optimizations applied. High cost warnings may persist in Cloud Trace.")
    print("="*50 + "\n")

if __name__ == "__main__":
    target = sys.argv[1] if len(sys.argv) > 1 else "src/backend/agent.py"
    try:
        audit_agent(target)
    except KeyboardInterrupt:
        print("\n\nAudit aborted.")
        sys.exit(1)
